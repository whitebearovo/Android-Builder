{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whitebearovo/Android-Builder/blob/main/sakura_14b_qwen2_5_v1_0_q6k_gguf_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Keep this widget playing to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "#@markdown Press play on the audio player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "Qwky911EHjUD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "84551f17-44cf-42fa-9706-aee821efc99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "Mount_GDrive = True # @param {type:\"boolean\"}\n",
        "if Mount_GDrive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  ROOT_PATH = \"/content/gdrive/MyDrive\"\n",
        "else:\n",
        "  ROOT_PATH = \"/content\"\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "3MMulGzF4G0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install --upgrade pip\n",
        "!pip install transformers tokenizers\n",
        "!pip install vllm huggingface-hub flask pyngrok triton torch"
      ],
      "metadata": {
        "id": "-n9gAyORR-et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run API backend (vLLM)\n",
        "# Use ngrok for API mapping\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Set the ngrok authentication token\n",
        "ngrokToken = \"2pR573GJUUUuRlAPKFQcKemTJk7_2uisJCJJ8ng8ERcwBexTp\"\n",
        "\n",
        "if ngrokToken:\n",
        "    from pyngrok import conf, ngrok\n",
        "\n",
        "    # Configure the ngrok authentication token\n",
        "    conf.get_default().auth_token = ngrokToken\n",
        "    conf.get_default().monitor_thread = False\n",
        "\n",
        "    # Start ngrok tunnel with the custom domain\n",
        "    try:\n",
        "        ssh_tunnel = ngrok.connect(8001, bind_tls=True, hostname=\"devoted-hen-awaited.ngrok-free.app\")\n",
        "        public_url = ssh_tunnel.public_url\n",
        "        print('Custom Domain Address: ' + public_url)\n",
        "    except Exception as e:\n",
        "        print(f\"Error starting ngrok tunnel: {e}\")\n",
        "else:\n",
        "    # Download cloudflared tunnel, use cloudflare to set up temporary tunnel (Not Static URL)\n",
        "    %cd $ROOT_PATH\n",
        "\n",
        "    !wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "    !chmod a+x cloudflared\n",
        "    !./cloudflared tunnel --url localhost:8001\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "%cd $ROOT_PATH\n",
        "\n",
        "# Download the model for the first time\n",
        "!HF_ENDPOINT=https://huggingface.co huggingface-cli download SakuraLLM/Sakura-14B-Qwen2.5-v1.0-GGUF --local-dir models --include sakura-14b-qwen2.5-v1.0-q6k.gguf\n",
        "\n",
        "!RAY_memory_monitor_refresh_ms=\"0\" HF_ENDPOINT=https://huggingface.co OMP_NUM_THREADS=36 VLLM_ATTENTION_BACKEND=XFORMERS vllm serve ./models/sakura-14b-qwen2.5-v1.0-q6k.gguf --tokenizer Qwen/Qwen2.5-14B-Instruct --dtype float16 --api-key token-abc123 --kv-cache-dtype auto --max-model-len 4096 --tensor-parallel-size 1 --gpu-memory-utilization 0.99 --disable-custom-all-reduce --enforce-eager --use-v2-block-manager --disable-log-requests --host 0.0.0.0 --port 8001 --served-model-name \"Qwen2.5-14B-Instruct\" &\n",
        "\n"
      ],
      "metadata": {
        "id": "xhitDqX21NTs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}